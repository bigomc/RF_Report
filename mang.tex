

\subsection{Soving the inverse kinematics of Lynxmotion by neural network (Mang Ning)}
\subsubsection{Neural network and back porpagation}
In essense, artificial neural network is a computational model that can approximate any functions. The structure of a network is formed by connecting each neuron through linear equation and specific activation function,which is expressed in equation (\ref{eq:nn_linear_equation}). Basically, the meaning of training a neural network is actually adjusting the variables $w$ and $b$ insides the linear function.  

\begin{equation}
\label{eq:nn_linear_equation}
a = \sigma(w_1a_1+w_2a_2+w_3a_3+...+w_na_n-b)
\end{equation}

Afterwards, our target is to minimize a cost function that measures the performance of the network . This function involves all variavles $w_i$ and $b_i$ which enable us to find the minimum value by computing the partial derivative   in terms of each $w_i$ and $b_i$. Then the cost function can be minimized along the partial derivative path just as shown in figure \ref{fig:gradient_descent}.

\begin{figure}[htbp] 
\begin{center}
\includegraphics[width=\textwidth]{images/gradient_descent}
\caption{the process of reaching the minimum point of cost function by gradient descent}
\label{fig:gradient_descent}
\end{center}
\end{figure}

Next, back propagation algorithm can derive a specific variation for every $w_i$ and $b_i$ in each iteration. As a result, the training of a neural network will be completed and it can be used to make predictions \citep{rumelhart1986learning}.  

\subsubsection{Applying neural network on inverse Kinematics}
As for the structure of this neural network applied on inverse kinematics,the input should be 12 neurons since the rotation and translation have 12 elements totally. Similarly, the output layer has 5 neurons for $\theta_1...\theta_5$(ignoring the redundant sulotion). Experientially, the number of hidden layers should be comparitive to the number of neurons of input layer. Thus, the structure can be expressed as figure \ref{fig:IK_neural_network}. 

\begin{figure}[htbp] 
\begin{center}
\includegraphics[width=\textwidth]{images/IK_neural_network}
\caption{the structure of the neural network applied in inverse kinematics}
\label{fig:IK_neural_network}
\end{center}
\end{figure}

A massive training data can be generated by previous forward kinematics code. Concretely, $10^5$ data is created based on picking $10$ values uniformly for $\theta_1...\theta_5$ respectively. These data will be split into traning data and test data at a ratio of $0.8$.
After setting relevant learning parameters, the training of neural network can be excuted. For simplicity, we only adopt $\theta_3$ as the output because the training time will exceed hundreds hours if the network output $\theta_1$ to $\theta_5$ simultaneously. Besides, the range of joints angle is also contrained between $[0^{\circ},18^{\circ}]$. 
It turns out that the training time is near 3 hours even under many constrains, but the level of accuracy is good enough to be accepted, which is less than $0.1^{\circ}$. All of the training details and outcome can be seen from figure \ref{fig:neural_network_outcome}.

\begin{figure}[htbp] 
\begin{center}
\includegraphics[width=\textwidth]{images/neural_network_outcome}
\caption{training outcome and prediction }
\label{fig:neural_network_outcome}
\end{center}
\end{figure}


\subsubsection{Conclusion and future study}
It can be concluded from massive experiments that the training time and accuracy will rise significantly with the increasement of network complexity and data volume. It is a robust way to calculate the inverse kinematics for a complexe robot with diverse joints and it might be the most feasible appraoch when solutions are hard to find by analytical method. However, the choice of network structure and parameters adjustment can also be time-consuming as they are generally determined by practical experience.

Actually, gradient descent is not an efficient algorithm to minimize the cost function when the shape of cost function is not quite smooth. Usually, it reachs the minimum point in a zigzage line (shown in figure  \ref{fig:slow}). In order to accelerate the training process, momentum gradient descent is applied in training neural networks. Compared with gradient descent, $\Delta w$ and $\Delta b$ in momentum algorithm are computed by considering the previous iteration and current iteration synthetically, relevant equations are presented below \citep{qian1999momentum}. Thus, the decreasing path will be smooth and more directily. This algorithm inspires me to opimize the neural network further.   

\begin{equation}
\Delta w_i = \beta \Delta v_w + (1-\beta)\Delta w_{i-1}
\end{equation}
\begin{equation}
\Delta b_i = \beta \Delta v_b + (1-\beta)\Delta b_{i-1}
\end{equation}

\begin{figure}[htbp] 
\begin{center}
\includegraphics[width=\textwidth]{images/slow}
\caption{the decreasing path of cost funtion by gradient descent algorithm}
\label{fig:slow}
\end{center}
\end{figure}


